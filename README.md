# iris.py コード解説

`iris.py`は機械学習の入門として有名なアイリス（アヤメ）データセットを使った分析を行うためのPythonプログラムです。この解説では、コード全体の構造、各クラスの役割、そして実際にどのように動作するのかを詳細に説明します。

## 目次

1. [プログラムの概要](#プログラムの概要)
2. [主要なクラスとその役割](#主要なクラスとその役割)
   - [FileHandler クラス](#1-filehandler-クラス)
   - [DataLoader クラス](#2-dataloader-クラス)
   - [Visualizer クラス](#3-visualizer-クラス)
   - [ScalingReporter クラス](#4-scalingreporter-クラス)
   - [ModelHandler クラス](#5-modelhandler-クラス)
   - [DataAnalyzer クラス](#6-dataanalyzer-クラス)
   - [IrisAnalyzer クラス](#7-irisanalyzer-クラス)
3. [エラーハンドリングの仕組み](#エラーハンドリングの仕組み)
4. [プログラムの実行フロー](#プログラムの実行フロー)
5. [各分析手法の詳細解説](#各分析手法の詳細解説)
   - [相関分析](#1-相関分析)
   - [ペアプロット分析](#2-ペアプロット分析)
   - [教師あり学習モデルの評価](#3-教師あり学習モデルの評価)
   - [特徴量重要度の分析](#4-特徴量重要度の分析)
   - [次元削減技法](#5-次元削減技法)
   - [クラスタリング技法](#6-クラスタリング技法)
6. [データ前処理テクニック](#データ前処理テクニック)
7. [データの解釈と分析フロー](#データの解釈と分析フロー)
8. [オブジェクト指向設計の利点](#オブジェクト指向設計の利点)
9. [リンター警告と改善点](#リンター警告と改善点)
10. [結論](#結論)
11. [使用方法と実行例](#使用方法と実行例)
12. [必要なライブラリとその役割](#必要なライブラリとその役割)

## プログラムの概要

このプログラムは、データの読み込み、前処理、可視化、機械学習モデルの適用まで、データ分析の一連の流れを自動化しています。特徴は以下の通りです：

1. **モジュラー設計**: 機能ごとに異なるクラスに分けられています
2. **エラーハンドリング**: 各処理でエラーが発生しても全体が停止しません
3. **多様な分析**: 教師あり学習、次元削減、クラスタリングなど様々な手法を適用できます
4. **可視化の充実**: 分析結果を様々なグラフで視覚的に確認できます
5. **対話型インターフェース**: メニュー方式で分析手法を選択できます
6. **拡張性**: 新しいデータセットや分析手法を容易に追加できます

## 主要なクラスとその役割

### 1. FileHandler クラス

このクラスは分析結果や生成されたグラフを保存する役割を担います。ファイル操作を一元管理することで、出力形式の統一性を保ち、コードの重複を避けています。

```python
class FileHandler:
    """ファイル操作を行うクラス"""
    
    def __init__(self, output_dir: str):
        """出力ディレクトリを指定して初期化"""
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)  # 指定ディレクトリが存在しない場合は作成
```

主要なメソッド：
- `save_figure()`: 生成されたグラフ（Matplotlib図）をファイルとして保存します。図の解像度（dpi）も指定可能で、グラフの余白を自動調整する機能も持っています。
- `save_data()`: データフレームをCSVファイルとして保存します。
- `ensure_dir()`: 指定されたディレクトリが存在することを確認し、必要に応じて作成します。

このクラスにより、可視化結果の保存プロセスが標準化され、一貫性のある出力が保証されます。

### 2. DataLoader クラス

データを読み込み、分析に適した形式に変換するクラスです。異なるデータソースからのデータ読み込みを抽象化し、統一したインターフェースを提供します。

```python
class DataLoader:
    """データ読み込みを行うクラス"""
    
    def __init__(self):
        """DataLoaderを初期化"""
        self.data = None       # 読み込んだデータフレーム全体
        self.X = None          # 特徴量（説明変数）のデータフレーム
        self.y = None          # ターゲット（目的変数）のシリーズ
        self.feature_names = None  # 特徴量の名前のリスト
        self.target_names = None   # クラスラベルの名前のリスト
```

主要なメソッド：
- `load_data()`: データセット名を指定して読み込むファサードメソッド。実際の処理は内部メソッドに委譲されます。
- `_load_iris()`: scikit-learnのloads_iris関数を使ってアイリスデータセットを読み込み、Pandasデータフレームに変換します。
- `_load_custom_csv()`: カスタムCSVファイルを読み込み、特徴量とターゲットに分割します。ヘッダーや区切り文字などの追加パラメータを指定可能です。
- `get_data()`: 読み込まれたデータの全体、特徴量（X）、ターゲット（y）をタプルとして返します。

このクラスは、データの前処理も担当しており、欠損値の処理や特徴量の型変換なども行います。また、エラー処理によりデータ読み込み時の問題に対しても堅牢に対応します。

### 3. Visualizer クラス

データ分析結果を可視化するためのクラスで、最も多機能なクラスの一つです。さまざまな可視化技術を集約し、分析結果を直感的に理解できるグラフを生成します。

```python
class Visualizer:
    """データ可視化を行うクラス"""
    
    def __init__(self, file_handler: FileHandler):
        """FileHandlerを使って初期化"""
        self.file_handler = file_handler
```

主要なメソッド：
- `plot_correlation()`: 特徴量間の相関係数を計算し、ヒートマップとして可視化します。色の強さで相関の強さを表現し、対角線上の冗長な情報をマスクして見やすくします。
- `plot_pair()`: 全ての特徴量の組み合わせに対して散布図を作成し、多次元データの関係性を一度に可視化します。対角線上には各特徴量の分布（ヒストグラムまたはカーネル密度推定）を表示します。
- `plot_feature_importances()`: 機械学習モデルが学習した特徴量の重要度を棒グラフで可視化します。複数のモデルの結果を同時に表示して比較できます。
- `plot_decision_tree()`: 決定木モデルの構造を木構造図として可視化し、分岐条件や各ノードのクラス分布を確認できます。
- `plot_dimension_reduction()`: PCA、NMF、t-SNEなどの次元削減手法の結果を2次元または3次元で可視化します。主成分の寄与率や特徴量の寄与度も表示可能です。
- `plot_clusters()`: K-means、DBSCANなどのクラスタリング結果を散布図で可視化します。実際のクラスラベルと比較することも可能です。
- `plot_dendrogram()`: 階層的クラスタリングの結果をデンドログラム（樹形図）として可視化します。クラスタ間の距離関係が直感的に理解できます。
- `plot_scaling_comparison()`: 異なるスケーリング手法を適用したデータを比較する散布図を作成します。

このクラスはMatplotlibとSeabornを組み合わせて高品質な可視化を実現しており、保存機能も統合されています。

### 4. ScalingReporter クラス

データスケーリング手法の性能比較レポートを作成します。異なるスケーリング手法がモデル性能にどのように影響するかを分析し、報告します。

```python
class ScalingReporter:
    """スケーリング手法の比較レポートを作成するクラス"""
    
    def __init__(self, file_handler: FileHandler):
        """FileHandlerを使って初期化"""
        self.file_handler = file_handler
```

主要なメソッド：
- `create_performance_summary()`: 各スケーリング手法の性能指標をデータフレームに集約し、全体的な性能傾向を可視化します。
- `_print_summary()`: 性能サマリーを整形してコンソールに表示します。平均精度、標準偏差などの統計情報が含まれます。
- `_save_summary()`: 性能サマリーをCSVファイルとして保存し、後での参照や比較を可能にします。

このクラスにより、データサイエンティストはどのスケーリング手法が特定の問題に最適かを判断する材料を得ることができます。

### 5. ModelHandler クラス

機械学習モデルの管理と評価を行います。複数のモデルを一元管理し、交差検証によるパフォーマンス評価を実施します。

```python
class ModelHandler:
    """機械学習モデルの管理と評価を行うクラス"""
    
    def __init__(self):
        """モデルハンドラーを初期化"""
        self.model_registry = {}  # モデル名をキー、モデルオブジェクトを値とする辞書
        self.model_scores = {}    # モデル評価結果を保存する辞書
        self._register_default_models()
```

主要なメソッド：
- `_register_default_models()`: LogisticRegression、SVC、RandomForestClassifierなど、一般的な分類モデルをデフォルトで登録します。
- `register_model()`: 新しいモデルをレジストリに追加します。カスタムモデルやハイパーパラメータをチューニングしたモデルを登録できます。
- `get_models_with_feature_importance()`: 特徴量重要度を提供するモデル（決定木ベースのモデルなど）のみを抽出します。
- `evaluate_models()`: K分割交差検証によりモデルのパフォーマンスを評価します。訓練データとテストデータでの精度を計算し、過学習の検出にも役立ちます。
- `get_model_results()`: 評価結果を整形して返します。モデル比較のためのデータフレームを生成します。
- `find_best_model()`: テスト精度が最も高いモデルとそのスコアを特定します。

このクラスは、モデル選択プロセスを自動化し、最適なモデルを客観的に評価する機能を提供します。

### 6. DataAnalyzer クラス

全ての機能を統合し、データ分析のワークフローを実行するメインクラスです。他のすべてのクラスを組み合わせて、データ分析の完全なパイプラインを提供します。

```python
class DataAnalyzer:
    """データ分析を実行するメインクラス"""
    
    def __init__(self, output_dir: str = CONFIG["output_dir"]):
        """分析環境を初期化"""
        self.output_dir = output_dir
        self.file_handler = FileHandler(output_dir)
        self.visualizer = Visualizer(self.file_handler)
        self.data_loader = DataLoader()
        self.model_handler = ModelHandler()
        self.scaling_reporter = ScalingReporter(self.file_handler)
        
        # データ保存用変数
        self.data = None  # データフレーム全体
        self.X = None     # 特徴量
        self.y = None     # ターゲット
```

主要なメソッド：
- `load_dataset()`: データローダーを使用してデータセットを読み込み、基本的な情報（形状、クラス分布など）を表示します。
- `_check_data_loaded()`: データが正常に読み込まれているかを確認する内部メソッドです。
- `get_correlation()`: 特徴量間の相関行列を計算し、ヒートマップを生成します。
- `pair_plot()`: 特徴量ペアの散布図と分布を可視化します。クラスごとに色分けされます。
- `all_supervised()`: 全ての登録済みモデル（または指定されたモデル）を評価します。交差検証の結果を返します。
- `get_supervised()`: モデル評価の結果をデータフレーム形式で取得します。モデル比較を容易にします。
- `best_supervised()`: 最も性能の高いモデルとそのスコアを特定します。
- `plot_feature_importances_all()`: 特徴量重要度を持つ全てのモデルについて、重要度を可視化します。
- `visualize_decision_tree()`: 決定木モデルの構造を可視化します。決定境界や分岐条件を理解するのに役立ちます。
- `plot_scaled_data()`: 異なるスケーリング手法を適用したデータを比較します。各スケーリング手法の特性と影響を視覚的に確認できます。
- `plot_pca()`, `plot_nmf()`, `plot_tsne()`: 異なる次元削減手法を適用し、結果を可視化します。データの構造や分離性を低次元で確認できます。
- `plot_k_means()`, `plot_dendrogram()`, `plot_dbscan()`: 異なるクラスタリング手法を適用し、結果を可視化します。教師なし学習によるデータのグループ化を分析できます。

このクラスはデータ分析の全過程をカバーする包括的なインターフェースを提供し、異なる手法を簡単に適用・比較できるようにします。

### 7. IrisAnalyzer クラス

`DataAnalyzer`を継承して、アイリスデータセット専用の分析クラスとして機能します。アイリスデータセットに特化した機能や設定を提供します。

```python
class IrisAnalyzer(DataAnalyzer):
    """アイリスデータセット用の分析クラス"""
    
    def load_dataset(self) -> pd.DataFrame:
        """アイリスデータセットを読み込む"""
        return super().load_dataset("iris")
```

このクラスは継承を活用した簡潔な実装例であり、特定のデータセットに特化した分析クラスを簡単に作成できることを示しています。`load_dataset()`メソッドをオーバーライドすることで、パラメータなしでアイリスデータセットを読み込めるようにしています。

## エラーハンドリングの仕組み

`error_handler`デコレータは、関数の実行中に発生する可能性のあるエラーを捕捉し、プログラム全体が停止するのを防ぎます。

```python
def error_handler(func):
    """関数のエラーハンドリングを行うデコレータ"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            print(f"エラーが発生しました: {e}")
            traceback.print_exc()
            return None
    return wrapper
```

`@wraps(func)`は、デコレータが適用された関数のメタデータ（関数名、ドキュメント文字列など）を保持する役割があります。

## プログラムの実行フロー

プログラムは`main()`関数から実行されます：

```python
def main():
    """メインプログラム実行関数"""
    print("Iris データ分析プログラム")
    print("=" * 50)
    
    # アナライザーの初期化
    analyzer = IrisAnalyzer()
    
    # データセット読み込み
    analyzer.load_dataset()
    
    # メインメニュー
    while True:
        print("\nメニュー:")
        print("1: データ概要を表示")
        print("2: ペアプロット（ヒストグラム付き）を表示")
        # ... 他のメニュー項目
        
        try:
            choice = int(input("\n選択してください: "))
        except ValueError:
            print("数値を入力してください。")
            continue
        
        if choice == 0:
            break
            
        # 以下、選択に応じた処理を実行...
```

ユーザーのメニュー選択に応じて、対応する分析が実行されます。

## 各分析手法の詳細解説

### 1. 相関分析

`plot_correlation()`メソッドでは、特徴量間の相関関係を可視化します。

- **Pearson相関係数**：2つの変数間の線形関係の強さを-1から1の範囲で表します
- **ヒートマップ**：相関の強さを色で表現し、視覚的に把握しやすくします
- **上三角マスキング**：対称行列なので、冗長な情報を隠して見やすくします

この分析により、例えばアイリスデータセットでは花弁の長さと幅に強い正の相関があることなどが分かります。

### 2. ペアプロット分析

`plot_pair()`メソッドでは、すべての特徴量のペアごとの散布図と、対角線上に各特徴量の分布を表示します。

- **散布図**：2つの特徴量間の関係性をプロット
- **色分け（hue）**：クラスによって点の色を変えることで、クラス間の差異を可視化
- **対角線（diag_kind）**：ヒストグラムやカーネル密度推定で各特徴量の分布を表示

ペアプロットは多次元データの相互関係を一度に確認できる強力な可視化ツールです。アイリスデータでは、品種間の特徴の違いが一目で分かります。

### 3. 教師あり学習モデルの評価

`evaluate_models()`メソッドでは、複数の機械学習モデルをクロスバリデーションで評価します。

- **k分割交差検証**：データを5つに分割し、4つを訓練、1つをテストに使用するプロセスを繰り返す
- **データ分割**：`iloc`でインデックスベースのスライシングを行う
- **精度計算**：`accuracy_score`で予測と実際のラベルを比較

この手法により、モデルの平均的な性能と分散を算出でき、過学習していないか確認できます。

### 4. 特徴量重要度の分析

`plot_feature_importances()`メソッドでは、各モデルが学習した特徴量の重要度を可視化します。

- **決定木ベース**：Random ForestやGradient Boostingでは`feature_importances_`属性から取得
- **線形モデル**：LogisticRegressionなどでは`coef_`属性（係数）の絶対値を使用
- **可視化**：重要度順にソートして水平棒グラフでプロット

この分析によって、どの特徴がモデルの予測に最も寄与しているかを理解できます。

### 5. 次元削減技法

#### PCA（主成分分析）

`plot_pca()`メソッドでは、高次元データを低次元に圧縮します。

- **スケーリング**：特徴量のスケールを揃えるために標準化（平均0、分散1）
- **主成分**：データの分散を最大化する方向（固有ベクトル）に投影
- **寄与率**：各主成分がどれだけデータの分散を説明するか

PCAはデータの構造を保ったまま次元を削減できる手法です。

#### t-SNE

`plot_tsne()`メソッドでは、類似したデータポイントを近くに、異なるポイントを遠くに配置する非線形次元削減を行います。

- **確率分布**：高次元空間での類似度を確率分布として表現
- **KLダイバージェンス**：高次元と低次元の確率分布の差を最小化
- **視覚化**：クラスタ構造が視覚的に明確になる

t-SNEは特にクラスタ構造の可視化に優れていますが、計算コストが高いです。

### 6. クラスタリング技法

#### K-means

`plot_k_means()`メソッドでは、データポイントをk個のクラスタに分類します。

- **中心点**：各クラスタの中心（セントロイド）を計算
- **割り当て**：各データポイントを最も近い中心点のクラスタに割り当て
- **更新**：クラスタ内のデータポイントの平均を取り、新しい中心点とする
- **収束**：中心点が変化しなくなるまで繰り返す

K-meansは実装が簡単で高速ですが、クラスタ数を事前に指定する必要があります。

#### DBSCAN

`plot_dbscan()`メソッドでは、密度ベースのクラスタリングを行います。

- **コアポイント**：指定半径`eps`内に`min_samples`以上のポイントを持つ点
- **直接到達可能**：コアポイントから距離`eps`以内にある点
- **クラスタ形成**：互いに直接到達可能なポイントのグループ
- **ノイズ**：どのクラスタにも属さないポイント（-1でラベル付け）

DBSCANは任意の形状のクラスタを検出でき、事前にクラスタ数を指定する必要がなく、外れ値も検出できます。

## データ前処理テクニック

### スケーリング手法

`plot_scaling_comparison()`メソッドでは、異なるスケーリング手法を比較します。

```python
scalers = {
    "StandardScaler": StandardScaler(),
    "MinMaxScaler": MinMaxScaler(),
    "RobustScaler": RobustScaler(),
    "Normalizer": Normalizer(),
    "None": None,
}
```

- **StandardScaler**：平均0、分散1に変換（z-score正規化）
- **MinMaxScaler**：最小値0、最大値1の範囲に変換
- **RobustScaler**：中央値と四分位範囲を使用し、外れ値に頑健なスケーリング
- **Normalizer**：サンプルごとにL2ノルムが1になるよう正規化

スケーリングはモデルの性能に大きく影響します。特に距離ベースのアルゴリズム（K-NN、SVM、K-means）では重要です。

## データの解釈と分析フロー

アイリスデータセットを例に、一般的な分析フローを解説します：

1. **データ探索**：`load_dataset()`でデータを読み込み、形状やクラス分布を確認
2. **可視化**：`pair_plot()`や`plot_correlation()`で特徴間の関係を把握
3. **次元削減**：`plot_pca()`などで高次元データを視覚化
4. **モデル選択**：`all_supervised()`で様々なモデルを評価し比較
5. **モデル解釈**：`plot_feature_importances()`や`plot_decision_tree()`でモデルの動作を理解
6. **クラスタリング**：`plot_k_means()`などで教師なし学習によるパターン発見

このフローにより、データの特性を理解し、最適なモデルを選択できます。

## オブジェクト指向設計の利点

このコードはオブジェクト指向プログラミング（OOP）の原則に従って設計されています：

1. **カプセル化**：各クラスが特定の責務を持ち、内部実装を隠蔽
2. **継承**：`IrisAnalyzer`が`DataAnalyzer`を継承し、機能を拡張
3. **依存性注入**：`Visualizer`が`FileHandler`を注入され、柔軟性を高める
4. **単一責任の原則**：各クラスが単一の役割を持つ設計

この設計により、コードの保守性と拡張性が向上しています。例えば、新しいデータセット用のアナライザーを追加するには、`DataAnalyzer`を継承するだけで済みます。

## リンター警告と改善点

コードにはいくつかのリンター警告が含まれていますが、主に以下の3種類です：

1. **インポート順序の問題**：Pythonのスタイルガイドでは、標準ライブラリのインポートが先、その後サードパーティライブラリという順序を推奨しています。

2. **型ヒントの問題**：`None`値を型ヒントでどのように扱うかに関する警告が多く見られます。例えば、`diag_kind: str = None`と書かれていますが、`None`は`str`型ではないため、より正確には`Optional[str] = None`とすべきです。

3. **その他の警告**：例えば`scatter`変数が定義されているが使われていない、などの警告があります。

### 改善のアイデア

1. **型ヒントの修正**：`typing.Optional`を使用して`None`を許容する型ヒントを修正
2. **インポート順序の整理**：標準ライブラリ、サードパーティライブラリの順に整理
3. **ドキュメント文字列の追加**：まだ不足している箇所にも詳細なドキュメント文字列を追加
4. **コンフィグの外部化**：`CONFIG`ディクショナリを外部ファイルや環境変数から読み込むように変更
5. **テストの追加**：各クラスの機能をテストするユニットテストの実装

## 結論

`iris.py`は機械学習の一連のワークフローを実装した教育的な目的のコードです。データの読み込みから前処理、モデル評価、可視化まで、データサイエンスの各ステップがモジュール化されています。オブジェクト指向設計により、コードの再利用性と拡張性が高まっています。

このコードは特にデータサイエンスの学習者にとって、以下の点で価値があります：
1. 異なる機械学習アルゴリズムの動作や結果を比較できる
2. 可視化手法が充実しており、データからの洞察を得やすい
3. 完全なデータ分析パイプラインの実装例として学ぶことができる
4. エラーハンドリングの仕組みが組み込まれており、堅牢なコードの書き方が学べる

このコードを理解し修正していくことで、Pythonプログラミング、機械学習アルゴリズム、データ可視化、エラーハンドリングなど、多岐にわたるスキルを向上させることができます。

## 使用方法と実行例

プログラムは以下のように実行できます：

```python
# プログラムの実行
if __name__ == "__main__":
    main()
```

実行すると、対話式のメニューが表示され、以下のような分析オプションから選択できます：

1. **データ概要の表示**: データの基本統計量、欠損値の確認など
2. **ペアプロットの表示**: 特徴量間の関係を可視化
3. **モデル評価**: 複数の機械学習モデルを評価し比較
4. **次元削減**: PCA、t-SNEなどで次元削減と可視化
5. **クラスタリング**: K-means、DBSCANなどでクラスタリング分析

### 実行例

アイリスデータを使った典型的な分析フローの例：

1. **データ読み込みと確認**
   ```
   > python iris.py
   Iris データ分析プログラム
   ==================================================
   データセット 'iris' を読み込んでいます...
   データ形状: (150, 7)
   クラス分布:
   0    50
   1    50
   2    50
   Name: target, dtype: int64
   ```

2. **ペアプロットの生成**
   ```
   メニュー:
   1: データ概要を表示
   2: ペアプロット（ヒストグラム付き）を表示
   > 2
   ペアプロットを作成しています...
   ペアプロットを保存しました: outputs/pairplot_hist.png
   ```

3. **モデル評価の実行**
   ```
   メニュー:
   4: 教師あり学習モデル評価
   > 4
   モデルを評価しています...
   
   LogisticRegressionの評価中...
     Fold 1: 訓練精度=0.9800, テスト精度=0.9667
     ...
     平均: 訓練精度=0.9767, テスト精度=0.9667 (±0.0333)
   ...
   ```

## 必要なライブラリとその役割

このプログラムを実行するためには、以下のライブラリが必要です：

1. **NumPy**: 数値計算の基盤ライブラリ。高速な配列操作や線形代数計算を提供します。
   ```python
   import numpy as np
   ```

2. **Pandas**: データ分析とデータ操作のためのライブラリ。データフレーム構造を使ってデータを効率的に処理します。
   ```python
   import pandas as pd
   ```

3. **Matplotlib**: データ可視化の基本ライブラリ。様々なグラフやプロットを作成できます。
   ```python
   import matplotlib.pyplot as plt
   ```

4. **Seaborn**: Matplotlibをベースにした統計データ可視化ライブラリ。より洗練されたグラフを簡単に作成できます。
   ```python
   import seaborn as sns
   ```

5. **Scikit-learn**: 機械学習のための包括的なライブラリ。モデルの実装、評価、前処理、次元削減など多様な機能を提供します。
   ```python
   from sklearn.datasets import load_iris
   from sklearn.model_selection import KFold
   from sklearn.metrics import accuracy_score
   from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer
   from sklearn.decomposition import PCA, NMF
   from sklearn.manifold import TSNE
   from sklearn.cluster import KMeans, DBSCAN
   ```

6. **SciPy**: 科学技術計算のためのライブラリ。階層的クラスタリングなどの機能を提供します。
   ```python
   from scipy.cluster.hierarchy import dendrogram, linkage
   ```

7. **標準ライブラリ**: Pythonの標準ライブラリからもいくつかのモジュールを使用しています。
   ```python
   import os
   import traceback
   from functools import wraps
   from typing import Any, Dict, List, Optional, Tuple, Union
   ```

各ライブラリは以下の主要な役割を担っています：
- **NumPy/Pandas**: データ構造と基本的な数値計算
- **Matplotlib/Seaborn**: データ可視化
- **Scikit-learn**: 機械学習モデル、前処理、評価
- **SciPy**: 特殊な数学的アルゴリズム（クラスタリングなど）
- **標準ライブラリ**: ファイル操作、エラー処理、型ヒントなど
