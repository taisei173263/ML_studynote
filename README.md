# アヤメデータを分析するための機械学習ツール

## はじめに

このプロジェクトは、機械学習の入門としてよく使われる「アヤメ（Iris）データセット」を分析するためのツールです。プログラミングや統計学の知識がなくても、様々な機械学習手法を試すことができます。このツールは異なる分析手法を比較しながら、機械学習の基本概念を実践的に学ぶことを目的としています。

## アヤメデータセットとは？

アヤメデータセットは3種類のアヤメの花（Setosa、Versicolor、Virginica）について、以下の4つの特徴を測定したデータです：

- がく片の長さ（sepal length）：花の外側の緑色の部分の長さ
- がく片の幅（sepal width）：花の外側の緑色の部分の幅
- 花弁の長さ（petal length）：花びらの長さ
- 花弁の幅（petal width）：花びらの幅

このデータセットには各種類50サンプルずつ、計150のサンプルが含まれています。これらの4つの特徴量（4次元のデータ）から、花の種類を予測したり、似た特徴を持つ花をグループ化したりする練習ができます。アヤメのデータセットは特徴量が少なく、クラス（アヤメの種類）がはっきり分かれているため、機械学習の入門に最適です。

## データの構造

データセットは以下のような構造になっています：

| がく片の長さ | がく片の幅 | 花弁の長さ | 花弁の幅 | ラベル（種類） |
|------------|----------|-----------|---------|--------------|
| 5.1        | 3.5      | 1.4       | 0.2     | Setosa       |
| 7.0        | 3.2      | 4.7       | 1.4     | Versicolor   |
| 6.3        | 3.3      | 6.0       | 2.5     | Virginica    |
| ...        | ...      | ...       | ...     | ...          |

## このツールでできること

### 1. データを見る・可視化する

データ分析の第一歩は、データの特性を視覚的に理解することです。このツールでは以下の可視化機能を提供します：

**ペアプロット（散布図行列）**: 
全ての特徴の組み合わせを散布図で表示し、データの全体像を把握できます。これにより、どの特徴量の組み合わせが種類の分類に役立つかを視覚的に確認できます。例えば、花弁の長さと幅の組み合わせではアヤメの種類がきれいに分かれることが分かります。

```python
iris = DataAnalyzer()
iris.pair_plot()  # デフォルトではヒストグラム表示
iris.pair_plot(diag_kind="kde")  # 対角線上にカーネル密度推定を表示
```

**相関行列**: 
各特徴間の相関係数を計算し、ヒートマップとして表示します。相関係数は-1から1の範囲の値で、1に近いほど正の相関（一方が増加すると他方も増加）、-1に近いほど負の相関（一方が増加すると他方は減少）を示します。0に近いほど相関がないことを示します。

```python
correlation = iris.get_correlation()
print(correlation)  # 相関係数の数値を確認
```

### 2. 教師あり学習（分類）

教師あり学習とは、「正解」のあるデータから学習して、新しいデータの種類を予測する方法です。このツールでは以下の分類アルゴリズムを実装し、比較できます：

#### ロジスティック回帰
- **仕組み**: 線形モデルを使って確率を計算し、確率に基づいてクラスを分類します
- **特徴**: 直線や平面で分類する最も基本的な方法で、確率値も出力できます
- **長所**: シンプルで解釈しやすく、オーバーフィット（過学習）しにくい
- **短所**: 複雑な非線形の関係を表現できない
- **適した用途**: 特徴量とクラスの関係が比較的単純な場合

#### サポートベクターマシン（SVM）
- **仕組み**: クラス間の距離（マージン）を最大化する決定境界を見つけます
- **種類**:
  - **LinearSVC**: 直線や平面で分類する線形SVMで、大規模データに効率的
  - **SVC**: カーネルトリックを使用して非線形の複雑な境界でも分類可能
- **長所**: 高次元データでも効果的で、適切なカーネルを選べば複雑なパターンを捉えられる
- **短所**: パラメータ調整が難しく、大規模データセットではSVCは計算コストが高い
- **適した用途**: 中小規模の複雑なデータセット

#### 決定木
- **仕組み**: 「もしA>3なら左、そうでなければ右」のような条件分岐を連続して、データを階層的に分割します
- **長所**: 結果を木の形で可視化でき、人間が理解しやすい。非線形関係も自然に捉えられる
- **短所**: 過学習しやすく、小さな変化に敏感になりがち
- **適した用途**: ルールベースの理解が必要な場合や、データの説明が重要な場合

#### k近傍法（KNN）
- **仕組み**: 新しいデータに最も近い既知のデータ点k個を見つけ、それらの多数決で分類します
- **長所**: シンプルで直感的。パラメータが少なく、実装が簡単
- **短所**: 計算コストが高く、特徴量のスケーリングに敏感
- **適した用途**: データ分布が不規則で、局所的な関係が重要な場合
- **パラメータ**: n_neighbors（近傍点の数）を変更して精度を調整できます

#### ランダムフォレスト
- **仕組み**: 複数の決定木を作成し、その予測を組み合わせる（アンサンブル手法）
- **長所**: 単一の決定木より安定し、過学習しにくい。特徴量の重要度も計算可能
- **短所**: 計算コストが高く、ブラックボックス的な要素がある
- **適した用途**: 高い予測精度が必要で、解釈性よりも性能が重視される場合

#### 勾配ブースティング
- **仕組み**: 弱い学習器（通常は浅い決定木）を順次追加し、前の学習器の誤差を修正していく
- **長所**: 非常に高い予測精度を実現できる
- **短所**: パラメータ調整が複雑で、訓練に時間がかかる
- **適した用途**: 競技や高精度が求められる予測タスク

#### ニューラルネットワーク（MLP）
- **仕組み**: 人間の脳のニューロンをモデル化した層状のネットワーク構造で学習
- **長所**: 複雑な非線形パターンを学習でき、大量のデータで高性能
- **短所**: 多くのパラメータ調整が必要で、オーバーフィットしやすい
- **適した用途**: 画像認識や自然言語処理のような複雑なデータで特に有効

#### 交差検証（5-fold交差検証）

このツールでは、5-fold交差検証を使用して各アルゴリズムの性能を評価しています。交差検証とは：

1. データセットを5つの等しいサイズの部分集合（フォールド）に分割
2. 各フォールドを順番にテストデータとして使用し、残りの4つを訓練データとして使用
3. 5回の試行の平均スコアを算出し、モデルの性能を評価

交差検証の利点：
- データの偏りに強く、より信頼性の高い評価が可能
- 全データを訓練とテストの両方に使用できる
- モデルの安定性を評価できる

すべての分類器を交差検証で比較するには：
```python
results = iris.all_supervised()
print(results)  # 各分類器の5-fold交差検証の結果
```

最も精度の高い分類器を確認するには：
```python
best_method, best_score = iris.best_supervised()
print(f"最良の分類器: {best_method}, スコア: {best_score:.4f}")
```

### 3. データの前処理（スケーリング）

機械学習の前に、データを適切な形に整えることが重要です。特に特徴量の値の範囲が大きく異なる場合、スケーリングが必要になります。例えば、「身長（cm）」と「体重（kg）」のように単位が異なる特徴量がある場合、値の範囲も大きく違います。このツールでは以下のスケーリング手法を比較できます：

#### MinMaxScaler（最小最大スケーリング）
- **仕組み**: データを0〜1の範囲に変換します
- **数式**: X_scaled = (X - X_min) / (X_max - X_min)
- **例**: 身長150cm〜190cmのデータがあった場合、150cmは0、190cmは1、170cmは0.5になります
- **長所**: 直感的で理解しやすく、各特徴の相対的な大きさを保持
- **短所**: 外れ値に敏感

#### StandardScaler（標準化スケーリング）
- **仕組み**: データの平均を0、標準偏差を1に変換します
- **数式**: X_scaled = (X - μ) / σ （μは平均、σは標準偏差）
- **例**: 平均身長170cm、標準偏差10cmの場合、170cmは0、180cmは1、160cmは-1になります
- **長所**: 多くの機械学習アルゴリズム（特に線形モデル）に適している
- **短所**: 外れ値がある場合は平均と標準偏差が歪む

#### RobustScaler（頑健スケーリング）
- **仕組み**: 中央値と四分位範囲（IQR）を使用したスケーリング
- **数式**: X_scaled = (X - median) / (Q3 - Q1) （Q1は第1四分位点、Q3は第3四分位点）
- **長所**: 外れ値に対して非常に堅牢（影響を受けにくい）
- **短所**: 狭い範囲に集中したデータでは差が強調されすぎる場合がある

#### Normalizer（正規化）
- **仕組み**: 各サンプル（行）のベクトル長を1に変換
- **数式**: X_scaled = X / ||X|| （||X||はベクトルのノルム）
- **長所**: サンプル間の方向（特徴量の比率）のみを考慮したい場合に有効
- **短所**: 絶対値の情報が失われる

このツールでは、各スケーリング手法をLinearSVCと組み合わせて評価し、その効果を視覚的に確認できます：

```python
scaling_results = iris.plot_scaled_data()
print(scaling_results)  # 各スケーリング手法の精度を表示
```

この結果から、アヤメデータセットではどのスケーリング手法が効果的かを判断できます。一般に：
- LinearSVC、ロジスティック回帰、SVMなどの線形モデルではStandardScalerが効果的
- k近傍法（KNN）ではMinMaxScalerが効果的な場合が多い
- 外れ値が多いデータセットではRobustScalerが適している

### 4. 次元削減・可視化

次元削減は、多次元データを少ない次元（通常2次元か3次元）に圧縮して、データの構造を可視化したり、計算効率を上げたりする手法です。アヤメデータセットは4次元ですが、より高次元のデータでは特に重要な技術です。

#### 主成分分析（PCA）
- **仕組み**: データの分散（ばらつき）が最大になる方向（主成分）を見つける統計的手法
- **特徴**: 線形変換で、元の次元間の相関を取り除いて直交する新しい軸を作成
- **解釈**: 第1主成分はデータの分散が最大となる方向、第2主成分は第1主成分と直交し分散が2番目に大きい方向
- **利点**: 計算が効率的で、元の特徴とどう関連しているかを解釈しやすい
- **実装**:
  ```python
  X_scaled, df_pca, pca_model = iris.plot_pca()
  print(pca_model.explained_variance_ratio_)  # 各主成分の寄与率
  ```
- **結果の見方**: 
  - 散布図：各点が各サンプル、色が品種を表す
  - ヒートマップ：各主成分（行）に対する元の特徴（列）の寄与度

#### 非負値行列因子分解（NMF）
- **仕組み**: 非負の値だけでデータ行列を2つの非負行列の積に分解
- **特徴**: 加法的な性質を持ち、部分的な特徴の組み合わせとして解釈できる
- **用途**: 画像処理、テキスト分析（トピックモデリング）など
- **制約**: 元データは非負でなければならない
- **実装**:
  ```python
  X_scaled, df_nmf, nmf_model = iris.plot_nmf()
  ```

#### t-SNE（t-分布確率的近傍埋め込み）
- **仕組み**: 高次元空間での類似度関係を保ちながら低次元に埋め込む非線形手法
- **特徴**: 局所的な構造を保存するのに優れており、クラスター構造を視覚化するのに適している
- **パラメータ**: perplexity（近傍のサイズを制御するパラメータ）が重要
- **注意点**: 反復計算結果に依存し、大規模データでは計算コストが高い
- **実装**:
  ```python
  iris.plot_tsne(perplexity=30)  # perplexityパラメータを調整可能
  ```

これらの次元削減手法の違い：
- **PCA**: 大域的な構造（全体の分散）を捉える線形手法
- **NMF**: 非負の加法的部分的構造を捉える線形手法
- **t-SNE**: 局所的な構造（近傍関係）を捉える非線形手法

アヤメデータセットでは、PCACが最も解釈しやすく、各主成分が元の特徴とどう関連しているかを明確に示します。一方、t-SNEはクラスターをより明確に分離表示しますが、軸の意味を解釈するのは困難です。

### 5. クラスタリング（教師なし学習）

クラスタリングは、データをラベル（正解）を使わずに似た特徴を持つグループに分ける手法です。自動的にデータの自然な構造を発見するのに役立ちます。

#### K-means（K平均法）
- **仕組み**: 
  1. k個のクラスタ中心をランダムに初期化
  2. 各データ点を最も近いクラスタ中心に割り当て
  3. クラスタごとに新しい中心点（重心）を計算
  4. 収束するまで2-3を繰り返す
- **特徴**: データを指定した数（k）のグループに分ける最も基本的なクラスタリング手法
- **パラメータ**: クラスタ数kの指定が必要
- **長所**: シンプルで計算効率が良く、解釈しやすい
- **短所**: 球形のクラスタを前提とし、初期値に依存、外れ値に敏感
- **実装**:
  ```python
  kmeans = iris.plot_k_means(n_clusters=3)  # クラスタ数を指定
  print(kmeans.cluster_centers_)  # クラスタの中心点
  ```

#### 階層的クラスタリング
- **仕組み**: ボトムアップ方式では、各データ点を個別クラスタとして始め、最も近いクラスタを逐次統合
- **リンケージ方法**:
  - 単連結法（最短距離法）: 最も近い点間の距離
  - 完全連結法（最長距離法）: 最も遠い点間の距離
  - 平均連結法: クラスタ間の全点の平均距離
  - ウォード法: クラスタ統合時の分散増加量を最小化
- **特徴**: データの階層的なツリー構造をデンドログラム（樹形図）で表示
- **長所**: クラスタ数を事前に決める必要がなく、様々な粒度のクラスタを同時に把握可能
- **短所**: 計算コストが高く（O(n²logn)）、大規模データに不向き
- **実装**:
  ```python
  iris.plot_dendrogram(truncate=False)  # 完全なデンドログラムを表示
  ```
- **解釈**: デンドログラムの縦軸は距離（非類似度）、横軸はサンプル。任意の高さで切断することでクラスタを得られます

#### DBSCAN（密度ベースクラスタリング）
- **仕組み**: 密度の高い領域をクラスタとして検出する手法
- **主要概念**:
  - **コア点**: 半径eps内にminPts以上の点がある点
  - **境界点**: コア点ではないがコア点の近傍にある点
  - **ノイズ点**: どのクラスタにも属さない点
- **パラメータ**:
  - **eps**: 近傍を定義する距離の閾値
  - **min_samples**: コア点を定義するための最小点数
- **長所**: 
  - クラスタ数を事前に指定する必要がない
  - 任意の形状のクラスタを検出可能
  - ノイズ点を自動的に識別
- **短所**: 
  - パラメータ設定が難しい
  - 密度が大きく異なるクラスタを同時に検出しにくい
- **実装**:
  ```python
  dbscan = iris.plot_dbscan(eps=0.5, min_samples=5, scaling=True)
  ```
- **パラメータ調整**: 
  - **eps値が小さい**: より小さく、より多くのクラスタを検出
  - **eps値が大きい**: より大きく、より少ないクラスタを検出
  - **min_samples値が大きい**: ノイズ耐性が高まるが、小さなクラスタを見落とす可能性

このツールでは、アヤメデータセットの正解ラベル（iris.target）と比較して、各クラスタリングアルゴリズムの性能を評価しています。現実の教師なし学習では正解ラベルは使用しませんが、このデータセットでは学習目的で比較できます。

## 使い方の例

1. プログラムを実行：
```python
from iris import DataAnalyzer

# インスタンス作成
iris = DataAnalyzer()

# データの基本情報を確認
print(iris.get().head())  # 最初の5行を表示
print(iris.get().describe())  # 基本統計量を表示

# データを可視化
iris.pair_plot()  # 特徴量のペアプロットを表示

# 相関行列を確認
correlation = iris.get_correlation()
print(correlation)

# 次元削減でデータ構造を確認
X_scaled, df_pca, pca_model = iris.plot_pca()
print(f"第1主成分の寄与率: {pca_model.explained_variance_ratio_[0]:.4f}")
print(f"第2主成分の寄与率: {pca_model.explained_variance_ratio_[1]:.4f}")

# 分類アルゴリズムを比較（5-fold交差検証）
results = iris.all_supervised()

# 最も精度の高い分類アルゴリズムを特定
best_method, best_score = iris.best_supervised()
print(f"最良の分類器: {best_method}, スコア: {best_score:.4f}")

# 決定木を可視化して解釈
dt_model = iris.visualize_decision_tree()

# 特徴量の重要度を確認
iris.plot_feature_importances_all()

# 異なるスケーリング手法の効果を比較
scaling_results = iris.plot_scaled_data()

# クラスタリングでデータの自然な構造を発見
kmeans = iris.plot_k_means()
iris.plot_dendrogram()
dbscan = iris.plot_dbscan(eps=0.5, min_samples=5, scaling=True)
```

## 応用例と実験アイデア

以下のような実験をしてみると、機械学習の理解が深まります：

1. **パラメータの影響を調べる**:
   - k近傍法のn_neighborsパラメータを変更して精度の変化を観察
   - DBSCANのepsとmin_samplesを変えてクラスタリング結果を比較

2. **特徴量選択の実験**:
   - 4つの特徴量から2つだけを選んで分類精度を比較
   - 特徴量重要度の結果に基づいて特徴を選択し、精度への影響を確認

3. **交差検証の分析**:
   - 各fold（分割）ごとの精度を比較して、モデルの安定性を評価
   - 異なるseed値で交差検証を実行し、結果の再現性を確認

4. **次元削減の比較**:
   - PCA、NMF、t-SNEの結果を視覚的に比較
   - 次元削減後のデータを使って分類を行い、精度への影響を調査

## 必要なライブラリ

このプログラムを実行するには、以下のPythonライブラリが必要です：

```bash
pip install matplotlib numpy pandas seaborn scipy scikit-learn
```

## 学習ポイント

このツールを使うことで、以下の機械学習の基本概念を実践的に学べます：

1. **データの可視化と前処理の重要性**: 
   - データを理解するための可視化技術
   - スケーリングなどの前処理が精度に与える影響

2. **教師あり学習と教師なし学習の違い**:
   - 教師あり学習: ラベル（正解）を使って予測モデルを構築
   - 教師なし学習: ラベルなしでデータの自然な構造を発見

3. **様々な機械学習アルゴリズムの特徴と使い分け**:
   - 線形モデルと非線形モデルの違い
   - モデルの複雑さとオーバーフィット（過学習）のトレードオフ
   - 各アルゴリズムの長所短所を理解

4. **次元削減による複雑なデータの理解**:
   - 高次元データの可視化手法
   - データの構造やクラスター傾向の把握

5. **クラスタリングによるデータのグループ化**:
   - 類似データの自動グループ化
   - 異なるクラスタリングアルゴリズムの特性理解

6. **モデル評価手法**:
   - 交差検証によるモデル性能の公平な評価
   - 様々な評価指標（正解率、適合率、再現率など）の理解

データサイエンスの初心者の方は、各メソッドを順番に試して、パラメータを変えながら結果の違いを観察してみることをおすすめします。アヤメのデータセットを使った実験を通じて、より複雑な実世界のデータ分析に必要な基礎スキルを身につけることができます。
