{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7aab70a-4c79-48f0-97f0-4bbf32271389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# 対話型表示を有効にする\n",
    "plt.ion()\n",
    "\n",
    "class AnalyzeIris:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.feature_names = None\n",
    "        self.target_names = None\n",
    "        self.model_scores = {}\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"irisデータセットを取得し、必要な形式に変換する\"\"\"\n",
    "        iris = load_iris()\n",
    "        self.feature_names = iris.feature_names\n",
    "        self.target_names = iris.target_names\n",
    "        \n",
    "        # データフレームに変換\n",
    "        self.data = pd.DataFrame(data=iris.data, columns=self.feature_names)\n",
    "        self.data['target'] = iris.target\n",
    "        self.data['species'] = self.data['target'].map({\n",
    "            0: self.target_names[0],\n",
    "            1: self.target_names[1],\n",
    "            2: self.target_names[2]\n",
    "        })\n",
    "        \n",
    "        # X, yに分割\n",
    "        self.X = self.data[self.feature_names]\n",
    "        self.y = self.data['target']\n",
    "        \n",
    "        return self.data\n",
    "\n",
    "    def get_correlation(self):\n",
    "        \"\"\"変数間の相関係数を計算して表示する\"\"\"\n",
    "        corr = self.X.corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "        plt.title('Feature Correlation Matrix')\n",
    "        plt.show()\n",
    "        return corr\n",
    "\n",
    "    def pair_plot(self, diag_kind=None):\n",
    "        \"\"\"seabornを使ってpair_plotを表示する\"\"\"\n",
    "        # ユーザーのサンプルコードに基づいた実装\n",
    "        df = pd.DataFrame(data=self.X)\n",
    "        df.columns = self.feature_names\n",
    "        df['species'] = [self.target_names[i] for i in self.y]\n",
    "        \n",
    "        if diag_kind:\n",
    "            # 対角成分をカーネル密度推定にする場合\n",
    "            sns.pairplot(df, hue='species', diag_kind='kde')\n",
    "        else:\n",
    "            # 通常のペアプロット\n",
    "            sns.pairplot(df, hue='species')\n",
    "        \n",
    "        plt.show()\n",
    "        return df\n",
    "\n",
    "    def all_supervised(self, n_neighbors=4):\n",
    "        \"\"\"複数の教師あり学習モデルを実行して評価する\"\"\"\n",
    "        # 評価対象のモデル\n",
    "        models = {\n",
    "            'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "            'LinearSVC': LinearSVC(max_iter=1000, dual='auto'),  # dual=Falseの代わりにautoを使用\n",
    "            'SVC': SVC(),\n",
    "            'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "            'KNeighborsClassifier': KNeighborsClassifier(n_neighbors=n_neighbors),\n",
    "            'LinearRegression': LinearRegression(),\n",
    "            'RandomForestClassifier': RandomForestClassifier(),\n",
    "            'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "            'MLPClassifier': MLPClassifier(max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        # K分割交差検証\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # 結果格納用\n",
    "        self.model_scores = {}\n",
    "        \n",
    "        # 各モデルに対して評価\n",
    "        for name, model in models.items():\n",
    "            print(f\"=== {name} ===\")\n",
    "            test_scores = []\n",
    "            train_scores = []\n",
    "            \n",
    "            for train_idx, test_idx in kf.split(self.X):\n",
    "                X_train, X_test = self.X.iloc[train_idx], self.X.iloc[test_idx]\n",
    "                y_train, y_test = self.y.iloc[train_idx], self.y.iloc[test_idx]\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # トレーニングスコア計算\n",
    "                if name == 'LinearRegression':\n",
    "                    train_score = model.score(X_train, y_train)\n",
    "                    test_score = model.score(X_test, y_test)\n",
    "                else:\n",
    "                    train_score = accuracy_score(y_train, model.predict(X_train))\n",
    "                    test_score = accuracy_score(y_test, model.predict(X_test))\n",
    "                \n",
    "                train_scores.append(train_score)\n",
    "                test_scores.append(test_score)\n",
    "                \n",
    "                print(f\"test score: {test_score:.3f}, train score: {train_score:.3f}\")\n",
    "            \n",
    "            # スコアを保存\n",
    "            self.model_scores[name] = {\n",
    "                'test_scores': test_scores,\n",
    "                'train_scores': train_scores,\n",
    "                'mean_test_score': np.mean(test_scores),\n",
    "                'mean_train_score': np.mean(train_scores)\n",
    "            }\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        return self.model_scores\n",
    "\n",
    "    def get_supervised(self):\n",
    "        \"\"\"学習結果をDataFrameで返す\"\"\"\n",
    "        if not self.model_scores:\n",
    "            return None\n",
    "        \n",
    "        results = []\n",
    "        for model_name, scores in self.model_scores.items():\n",
    "            for i, (test, train) in enumerate(zip(scores['test_scores'], scores['train_scores'])):\n",
    "                results.append({\n",
    "                    'model': model_name,\n",
    "                    'fold': i+1,\n",
    "                    'test_score': test,\n",
    "                    'train_score': train\n",
    "                })\n",
    "        \n",
    "        df_scores = pd.DataFrame(results)\n",
    "        return df_scores\n",
    "\n",
    "    def best_supervised(self):\n",
    "        \"\"\"最良のモデルを返す\"\"\"\n",
    "        if not self.model_scores:\n",
    "            return None, 0\n",
    "        \n",
    "        best_model = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for model_name, scores in self.model_scores.items():\n",
    "            mean_score = scores['mean_test_score']\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_model = model_name\n",
    "        \n",
    "        return best_model, best_score\n",
    "\n",
    "    def plot_feature_importances_all(self):\n",
    "        \"\"\"特徴量の重要度をプロットする\"\"\"\n",
    "        models_with_importances = [\n",
    "            'DecisionTreeClassifier', \n",
    "            'RandomForestClassifier', \n",
    "            'GradientBoostingClassifier'\n",
    "        ]\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        for i, model_name in enumerate(models_with_importances):\n",
    "            if model_name not in self.model_scores:\n",
    "                continue\n",
    "                \n",
    "            # モデルを再学習\n",
    "            if model_name == 'DecisionTreeClassifier':\n",
    "                model = DecisionTreeClassifier()\n",
    "            elif model_name == 'RandomForestClassifier':\n",
    "                model = RandomForestClassifier()\n",
    "            elif model_name == 'GradientBoostingClassifier':\n",
    "                model = GradientBoostingClassifier()\n",
    "            \n",
    "            model.fit(self.X, self.y)\n",
    "            \n",
    "            # 重要度をプロット\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            importance = model.feature_importances_\n",
    "            indices = np.argsort(importance)[::-1]\n",
    "            \n",
    "            plt.bar(range(len(self.feature_names)), importance[indices])\n",
    "            plt.xticks(range(len(self.feature_names)), [self.feature_names[i] for i in indices], rotation=90)\n",
    "            plt.title(f'{model_name} Feature Importance')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_decision_tree(self):\n",
    "        \"\"\"決定木を可視化する\"\"\"\n",
    "        tree = DecisionTreeClassifier()\n",
    "        tree.fit(self.X, self.y)\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plot_tree(tree, filled=True, feature_names=self.feature_names, class_names=self.target_names)\n",
    "        plt.title('Decision Tree Visualization')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def plot_scaled_data(self):\n",
    "        \"\"\"異なるスケーリング手法でデータを変換し、LinearSVCの結果を評価する\"\"\"\n",
    "        scalers = {\n",
    "            'Original': None,\n",
    "            'MinMaxScaler': MinMaxScaler(),\n",
    "            'StandardScaler': StandardScaler(),\n",
    "            'RobusScaler': RobustScaler(),\n",
    "            'Normalizer': Normalizer()\n",
    "        }\n",
    "        \n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        for train_idx, test_idx in kf.split(self.X):\n",
    "            X_train, X_test = self.X.iloc[train_idx], self.X.iloc[test_idx]\n",
    "            y_train, y_test = self.y.iloc[train_idx], self.y.iloc[test_idx]\n",
    "            \n",
    "            print(\"=\" * 73)\n",
    "            \n",
    "            for name, scaler in scalers.items():\n",
    "                if scaler:\n",
    "                    X_train_scaled = scaler.fit_transform(X_train)\n",
    "                    X_test_scaled = scaler.transform(X_test)\n",
    "                else:\n",
    "                    X_train_scaled = X_train\n",
    "                    X_test_scaled = X_test\n",
    "                \n",
    "                model = LinearSVC(max_iter=1000, dual='auto')  # dual=Falseの代わりにautoを使用\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                train_score = accuracy_score(y_train, model.predict(X_train_scaled))\n",
    "                test_score = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "                \n",
    "                print(f\"{name:<15}: test score: {test_score:.3f}      train score: {train_score:.3f}     \")\n",
    "        \n",
    "        print(\"=\" * 73)\n",
    "        \n",
    "        # 最初のスケーリングデータを返す (標準スケーリング)\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(self.X)\n",
    "        return pd.DataFrame(X_scaled, columns=self.feature_names)\n",
    "\n",
    "    def plot_pca(self, n_components=2):\n",
    "        \"\"\"PCA分析を行い結果をプロットする\"\"\"\n",
    "        # データのスケーリング\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(self.X)\n",
    "        \n",
    "        # PCA実行\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # 結果をデータフレーム化\n",
    "        df_pca = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "        df_pca['species'] = self.data['species']\n",
    "        \n",
    "        # プロット\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue='species', palette='viridis', s=100)\n",
    "        plt.title('PCA of Iris Dataset')\n",
    "        \n",
    "        # 主成分の寄与率\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        plt.xlabel(f'PC1 ({explained_variance[0]:.2f})')\n",
    "        plt.ylabel(f'PC2 ({explained_variance[1]:.2f})')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return pd.DataFrame(X_scaled, columns=self.feature_names), df_pca, pca\n",
    "\n",
    "    def plot_nmf(self, n_components=2):\n",
    "        \"\"\"NMF分析を行い結果をプロットする\"\"\"\n",
    "        # データのスケーリング (負の値は使えないのでMinMaxScalerを使用)\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(self.X)\n",
    "        \n",
    "        # NMF実行 (反復回数を増やして警告を減らす)\n",
    "        nmf = NMF(n_components=n_components, random_state=42, max_iter=400)\n",
    "        X_nmf = nmf.fit_transform(X_scaled)\n",
    "        \n",
    "        # 結果をデータフレーム化\n",
    "        df_nmf = pd.DataFrame(X_nmf, columns=[f'NMF{i+1}' for i in range(n_components)])\n",
    "        df_nmf['species'] = self.data['species']\n",
    "        \n",
    "        # プロット\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.scatterplot(data=df_nmf, x='NMF1', y='NMF2', hue='species', palette='viridis', s=100)\n",
    "        plt.title('NMF of Iris Dataset')\n",
    "        plt.show()\n",
    "        \n",
    "        return pd.DataFrame(X_scaled, columns=self.feature_names), df_nmf, nmf\n",
    "\n",
    "    def plot_tsne(self):\n",
    "        \"\"\"t-SNE分析を行い結果をプロットする\"\"\"\n",
    "        # t-SNE実行 (スケールしていない元データを使用)\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        X_tsne = tsne.fit_transform(self.X)\n",
    "        \n",
    "        # 結果をデータフレーム化\n",
    "        df_tsne = pd.DataFrame(X_tsne, columns=['t-SNE1', 't-SNE2'])\n",
    "        df_tsne['species'] = self.data['species']\n",
    "        \n",
    "        # プロット\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.scatterplot(data=df_tsne, x='t-SNE1', y='t-SNE2', hue='species', palette='viridis', s=100)\n",
    "        plt.title('t-SNE of Iris Dataset')\n",
    "        plt.show()\n",
    "        \n",
    "        return df_tsne\n",
    "\n",
    "    def plot_k_means(self):\n",
    "        \"\"\"K-means分析を行い結果をプロットする\"\"\"\n",
    "        # K-means実行\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "        clusters = kmeans.fit_predict(self.X)\n",
    "        \n",
    "        # 結果をデータフレーム化\n",
    "        df_kmeans = self.X.copy()\n",
    "        df_kmeans['cluster'] = clusters\n",
    "        df_kmeans['actual'] = self.y\n",
    "        \n",
    "        # プロット\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # K-means結果\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.scatterplot(x=self.X.iloc[:, 0], y=self.X.iloc[:, 1], hue=clusters, palette='viridis', s=100)\n",
    "        plt.title('K-means Clustering')\n",
    "        plt.xlabel(self.feature_names[0])\n",
    "        plt.ylabel(self.feature_names[1])\n",
    "        \n",
    "        # 実際のラベル\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.scatterplot(x=self.X.iloc[:, 0], y=self.X.iloc[:, 1], hue=self.y, palette='viridis', s=100)\n",
    "        plt.title('Actual Classes')\n",
    "        plt.xlabel(self.feature_names[0])\n",
    "        plt.ylabel(self.feature_names[1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"KMeans法で予測したラベル:\")\n",
    "        print(clusters)\n",
    "        print(\"\\n実際のラベル:\")\n",
    "        print(self.y.values)\n",
    "        \n",
    "        return df_kmeans\n",
    "\n",
    "    def plot_dendrogram(self, truncate=False):\n",
    "        \"\"\"階層的クラスタリングのデンドログラムをプロットする\"\"\"\n",
    "        try:\n",
    "            # リンケージ行列を計算 (DataFrameをnumpy arrayに変換)\n",
    "            X_array = self.X.values\n",
    "            linked = linkage(X_array, 'ward')\n",
    "            \n",
    "            # デンドログラムをプロット\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            dendrogram(\n",
    "                linked,\n",
    "                truncate_mode='lastp' if truncate else None,\n",
    "                p=10 if truncate else None,\n",
    "                leaf_font_size=10.,\n",
    "                orientation='top'\n",
    "            )\n",
    "            plt.title('Hierarchical Clustering Dendrogram')\n",
    "            plt.xlabel('Sample index')\n",
    "            plt.ylabel('Distance')\n",
    "            plt.show()\n",
    "            \n",
    "            return linked\n",
    "        except Exception as e:\n",
    "            print(f\"デンドログラムの作成中にエラーが発生しました: {e}\")\n",
    "            return None\n",
    "\n",
    "    def plot_dbscan(self, scaling=False, eps=0.5, min_samples=5):\n",
    "        \"\"\"DBSCAN分析を行い結果をプロットする\"\"\"\n",
    "        # データのスケーリング (オプション)\n",
    "        if scaling:\n",
    "            scaler = StandardScaler()\n",
    "            X_dbscan = scaler.fit_transform(self.X)\n",
    "        else:\n",
    "            X_dbscan = self.X.values\n",
    "        \n",
    "        # DBSCAN実行\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        clusters = dbscan.fit_predict(X_dbscan)\n",
    "        \n",
    "        # 結果をデータフレーム化\n",
    "        df_dbscan = self.X.copy()\n",
    "        df_dbscan['cluster'] = clusters\n",
    "        \n",
    "        try:\n",
    "            # 特徴量の組み合わせをプロット\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "            \n",
    "            # プロットのためのカラーマップ (-1はノイズ点で黒にする)\n",
    "            cmap = plt.cm.viridis\n",
    "            cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "            cmaplist[0] = (0, 0, 0, 1.0)  # ノイズ点を黒に\n",
    "            cmap_custom = plt.matplotlib.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "            \n",
    "            # 特徴量ペアのプロット\n",
    "            feature_pairs = [\n",
    "                (0, 1), (0, 2), (0, 3),\n",
    "                (1, 2), (1, 3), (2, 3)\n",
    "            ]\n",
    "            \n",
    "            for i, (f1, f2) in enumerate(feature_pairs):\n",
    "                row, col = i // 3, i % 3\n",
    "                axes[row, col].scatter(X_dbscan[:, f1], X_dbscan[:, f2], c=clusters, cmap=cmap_custom, s=50)\n",
    "                axes[row, col].set_xlabel(self.feature_names[f1])\n",
    "                axes[row, col].set_ylabel(self.feature_names[f2])\n",
    "                axes[row, col].set_title(f'{self.feature_names[f1]} vs {self.feature_names[f2]}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle('DBSCAN Clustering' + (' (Scaled)' if scaling else ''), y=1.02, fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"DBSCAN散布図の作成中にエラーが発生しました: {e}\")\n",
    "        \n",
    "        print(\"Cluster Memberships:\", clusters)\n",
    "        \n",
    "        return df_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1015ba81-b348-47df-9ac1-ecb322695f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Irisデータセット分析スクリプト =====\n",
      "Matplotlibのバックエンド: Agg\n",
      "AnalyzeIrisクラスのインスタンスを作成します...\n",
      "\n",
      "1. Irisデータセットの読み込み\n",
      "データサイズ: (150, 6)\n",
      "データサンプル表示():\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                 5.1               3.5                1.4               0.2   \n",
      "1                 4.9               3.0                1.4               0.2   \n",
      "2                 4.7               3.2                1.3               0.2   \n",
      "3                 4.6               3.1                1.5               0.2   \n",
      "4                 5.0               3.6                1.4               0.2   \n",
      "5                 5.4               3.9                1.7               0.4   \n",
      "6                 4.6               3.4                1.4               0.3   \n",
      "7                 5.0               3.4                1.5               0.2   \n",
      "8                 4.4               2.9                1.4               0.2   \n",
      "9                 4.9               3.1                1.5               0.1   \n",
      "10                5.4               3.7                1.5               0.2   \n",
      "11                4.8               3.4                1.6               0.2   \n",
      "12                4.8               3.0                1.4               0.1   \n",
      "13                4.3               3.0                1.1               0.1   \n",
      "14                5.8               4.0                1.2               0.2   \n",
      "15                5.7               4.4                1.5               0.4   \n",
      "16                5.4               3.9                1.3               0.4   \n",
      "17                5.1               3.5                1.4               0.3   \n",
      "18                5.7               3.8                1.7               0.3   \n",
      "19                5.1               3.8                1.5               0.3   \n",
      "\n",
      "    target species  \n",
      "0        0  setosa  \n",
      "1        0  setosa  \n",
      "2        0  setosa  \n",
      "3        0  setosa  \n",
      "4        0  setosa  \n",
      "5        0  setosa  \n",
      "6        0  setosa  \n",
      "7        0  setosa  \n",
      "8        0  setosa  \n",
      "9        0  setosa  \n",
      "10       0  setosa  \n",
      "11       0  setosa  \n",
      "12       0  setosa  \n",
      "13       0  setosa  \n",
      "14       0  setosa  \n",
      "15       0  setosa  \n",
      "16       0  setosa  \n",
      "17       0  setosa  \n",
      "18       0  setosa  \n",
      "19       0  setosa  \n",
      "\n",
      "2. 変数間の相関関係を確認\n",
      "相関行列を保存しました: output/correlation_matrix.png\n",
      "相関行列:\n",
      "                   sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
      "sepal length (cm)           1.000000         -0.117570           0.871754   \n",
      "sepal width (cm)           -0.117570          1.000000          -0.428440   \n",
      "petal length (cm)           0.871754         -0.428440           1.000000   \n",
      "petal width (cm)            0.817941         -0.366126           0.962865   \n",
      "\n",
      "                   petal width (cm)  \n",
      "sepal length (cm)          0.817941  \n",
      "sepal width (cm)          -0.366126  \n",
      "petal length (cm)          0.962865  \n",
      "petal width (cm)           1.000000  \n",
      "\n",
      "3. ペアプロット (対角線にヒストグラム)\n",
      "ペアプロットを保存しました: output/pairplot.png\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enterキーを押して続行... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. ペアプロット (対角線にカーネル密度推定)\n",
      "ペアプロットを保存しました: output/pairplot.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# このスクリプトはAnalyzeIrisクラスを実行するためのものです\n",
    "# iris.pyファイルと同じディレクトリに保存して実行してください\n",
    "\n",
    "import matplotlib\n",
    "from iris import AnalyzeIris\n",
    "\n",
    "def main():\n",
    "    print(\"===== Irisデータセット分析スクリプト =====\")\n",
    "    \n",
    "    # Matplotlibのバックエンドを確認\n",
    "    print(f\"Matplotlibのバックエンド: {matplotlib.get_backend()}\")\n",
    "    \n",
    "    print(\"AnalyzeIrisクラスのインスタンスを作成します...\")\n",
    "    iris = AnalyzeIris()\n",
    "    \n",
    "    # 1. データの読み込み\n",
    "    print(\"\\n1. Irisデータセットの読み込み\")\n",
    "    data = iris.get()\n",
    "    print(f\"データサイズ: {data.shape}\")\n",
    "    print(\"データサンプル:\")\n",
    "    print(data.head())\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 2. 相関分析\n",
    "    print(\"\\n2. 変数間の相関関係を確認\")\n",
    "    try:\n",
    "        correlation = iris.get_correlation()\n",
    "        print(\"相関行列:\")\n",
    "        print(correlation)\n",
    "    except Exception as e:\n",
    "        print(f\"相関行列の表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 3. ペアプロット (ヒストグラム)\n",
    "    print(\"\\n3. ペアプロット (対角線にヒストグラム)\")\n",
    "    try:\n",
    "        iris.pair_plot()\n",
    "    except Exception as e:\n",
    "        print(f\"ペアプロットの表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 4. ペアプロット (KDE)\n",
    "    print(\"\\n4. ペアプロット (対角線にカーネル密度推定)\")\n",
    "    try:\n",
    "        iris.pair_plot(diag_kind=\"kde\")\n",
    "    except Exception as e:\n",
    "        print(f\"KDEペアプロットの表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 5. 教師あり学習モデルの評価\n",
    "    print(\"\\n5. 複数の教師あり学習モデルを評価\")\n",
    "    print(\"9つのモデルをクロスバリデーションで評価します...\")\n",
    "    model_scores = iris.all_supervised(n_neighbors=4)\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 6. 学習結果の取得\n",
    "    print(\"\\n6. 学習結果をデータフレームで取得\")\n",
    "    df_scores = iris.get_supervised()\n",
    "    print(df_scores.head(15))  # 最初の数行を表示\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 7. 結果の要約統計量\n",
    "    print(\"\\n7. スコアの要約統計量\")\n",
    "    df_summary = df_scores.describe()\n",
    "    print(df_summary)\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 8. 最良のモデルを特定\n",
    "    print(\"\\n8. 最良のモデルを特定\")\n",
    "    best_method, best_score = iris.best_supervised()\n",
    "    print(f\"BestMethod is {best_method} : {best_score:.4f}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 9. 特徴量の重要度を可視化\n",
    "    print(\"\\n9. 特徴量の重要度をプロット\")\n",
    "    try:\n",
    "        iris.plot_feature_importances_all()\n",
    "    except Exception as e:\n",
    "        print(f\"特徴量重要度の表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 10. 決定木の可視化\n",
    "    print(\"\\n10. 決定木を可視化\")\n",
    "    try:\n",
    "        tree = iris.visualize_decision_tree()\n",
    "    except Exception as e:\n",
    "        print(f\"決定木の表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 11. データスケーリングとその効果\n",
    "    print(\"\\n11. 異なるスケーリング手法の効果を確認\")\n",
    "    print(\"各スケーリング手法でLinearSVCを評価します...\")\n",
    "    train_data = iris.plot_scaled_data()\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 12. PCA分析\n",
    "    print(\"\\n12. PCA (主成分分析)\")\n",
    "    try:\n",
    "        X_scaled, df_pca, X_pca = iris.plot_pca(n_components=2)\n",
    "        print(\"PCAの主成分:\")\n",
    "        print(X_pca.components_)\n",
    "    except Exception as e:\n",
    "        print(f\"PCA分析の表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 13. NMF分析\n",
    "    print(\"\\n13. NMF (非負値行列因子分解)\")\n",
    "    try:\n",
    "        X_scaled, df_nmf, X_nmf = iris.plot_nmf(n_components=2)\n",
    "    except Exception as e:\n",
    "        print(f\"NMF分析の表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 14. t-SNE分析\n",
    "    print(\"\\n14. t-SNE分析\")\n",
    "    try:\n",
    "        iris.plot_tsne()\n",
    "    except Exception as e:\n",
    "        print(f\"t-SNE分析の表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 15. K-means分析\n",
    "    print(\"\\n15. K-means分析\")\n",
    "    try:\n",
    "        df_kmeans = iris.plot_k_means()\n",
    "    except Exception as e:\n",
    "        print(f\"K-means分析の表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 16. 階層的クラスタリング (デンドログラム)\n",
    "    print(\"\\n16. 階層的クラスタリング (デンドログラム)\")\n",
    "    try:\n",
    "        iris.plot_dendrogram()\n",
    "    except Exception as e:\n",
    "        print(f\"デンドログラムの表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 17. 簡略化したデンドログラム\n",
    "    print(\"\\n17. 簡略化したデンドログラム\")\n",
    "    try:\n",
    "        iris.plot_dendrogram(truncate=True)\n",
    "    except Exception as e:\n",
    "        print(f\"簡略化デンドログラムの表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 18. DBSCAN分析\n",
    "    print(\"\\n18. DBSCAN分析\")\n",
    "    try:\n",
    "        df_dbscan = iris.plot_dbscan()\n",
    "    except Exception as e:\n",
    "        print(f\"DBSCAN分析の表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 19. スケーリングありのDBSCAN分析\n",
    "    print(\"\\n19. スケーリングありのDBSCAN分析\")\n",
    "    try:\n",
    "        df_dbscan_scaled = iris.plot_dbscan(scaling=True, eps=0.5, min_samples=5)\n",
    "    except Exception as e:\n",
    "        print(f\"スケーリングありDBSCAN分析の表示中にエラーが発生しました: {e}\")\n",
    "    input(\"Enterキーを押して続行...\")\n",
    "    \n",
    "    # 20. 最終課題\n",
    "    print(\"\\n20. 最終課題: クラスタリング手法の比較\")\n",
    "    print(\"\"\"\n",
    "    KMeans, 階層的クラスタリング, DBSCANの比較:\n",
    "    \n",
    "    1. K-means:\n",
    "       - 長所: 単純で理解しやすい、計算効率が良い\n",
    "       - 短所: クラスタ数を事前指定、球形クラスタのみ対応\n",
    "       - Irisデータでは: k=3で比較的適切に機能するが、完全には分離できない\n",
    "       \n",
    "    2. 階層的クラスタリング:\n",
    "       - 長所: クラスタ数事前指定不要、階層関係を視覚化\n",
    "       - 短所: 計算コスト高、大規模データに不向き\n",
    "       - Irisデータでは: 種の分類学的関係を明確に示す\n",
    "       \n",
    "    3. DBSCAN:\n",
    "       - 長所: クラスタ数事前指定不要、異常値検出、任意形状対応\n",
    "       - 短所: パラメータ設定が難しい、密度差のあるクラスタに弱い\n",
    "       - Irisデータでは: パラメータによって結果が大きく変わる\n",
    "       \n",
    "    結論: Irisデータセットでは種の数が既知なのでK-meansが単純かつ効果的ですが、\n",
    "    実際の応用では事前知識の有無によって最適手法が異なります。\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n===== 分析完了 =====\")\n",
    "    print(\"すべての分析ステップが終了しました。\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b747509-c55a-474a-b104-8ffc3d4638e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
